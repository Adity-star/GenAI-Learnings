{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 10782024,
          "sourceType": "datasetVersion",
          "datasetId": 6690431
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Beginner's Text Preprocessing",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing is an essential step in natural language processing (NLP) that involves preparing and cleaning text data before applying any machine learning or deep learning models. It ensures that the data is in a suitable format for analysis and modeling. Common text preprocessing techniques include:\n",
        "1. Lower casing\n",
        "2. Removal of HTML Tags\n",
        "3. Removal of URls\n",
        "4. Removal of Punctuations\n",
        "5. Removal of Stop words\n",
        "6. Removel of emojis\n",
        "7. Conversion of emojis to words\n",
        "8. Removel of frequent Words\n",
        "9. Stemming\n",
        "10. Lemmatization\n",
        "11. Chat words conversion\n",
        "12. Spelling correction"
      ],
      "metadata": {
        "id": "hu6XHcCQXukS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "pd.options.mode.chained_assignment = None"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:00:49.60654Z",
          "iopub.execute_input": "2025-02-18T10:00:49.607018Z",
          "iopub.status.idle": "2025-02-18T10:00:58.123644Z",
          "shell.execute_reply.started": "2025-02-18T10:00:49.606983Z",
          "shell.execute_reply": "2025-02-18T10:00:58.122503Z"
        },
        "id": "NxSwCH9yXukU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"/kaggle/input/twitter-dataset/sample.csv\", nrows=5000)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:01:17.516319Z",
          "iopub.execute_input": "2025-02-18T10:01:17.516764Z",
          "iopub.status.idle": "2025-02-18T10:01:17.527431Z",
          "shell.execute_reply.started": "2025-02-18T10:01:17.516727Z",
          "shell.execute_reply": "2025-02-18T10:01:17.526179Z"
        },
        "id": "hLcKOt3EXukV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:01:29.603708Z",
          "iopub.execute_input": "2025-02-18T10:01:29.604161Z",
          "iopub.status.idle": "2025-02-18T10:01:29.64272Z",
          "shell.execute_reply.started": "2025-02-18T10:01:29.60413Z",
          "shell.execute_reply": "2025-02-18T10:01:29.641403Z"
        },
        "id": "-wEoNWUDXukW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_text = df[['text']]\n",
        "df_text['text'] = df['text'].astype(str)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:02:22.36163Z",
          "iopub.execute_input": "2025-02-18T10:02:22.362087Z",
          "iopub.status.idle": "2025-02-18T10:02:22.375387Z",
          "shell.execute_reply.started": "2025-02-18T10:02:22.362051Z",
          "shell.execute_reply": "2025-02-18T10:02:22.374195Z"
        },
        "id": "ezlqbJLdXukW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_text.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:02:35.139257Z",
          "iopub.execute_input": "2025-02-18T10:02:35.139721Z",
          "iopub.status.idle": "2025-02-18T10:02:35.150754Z",
          "shell.execute_reply.started": "2025-02-18T10:02:35.139688Z",
          "shell.execute_reply": "2025-02-18T10:02:35.149264Z"
        },
        "id": "v--7GRSZXukX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Lower Casing\n",
        "Lower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way,its because python is case sensitive language.\n",
        "\n",
        "This is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts / tfidf values.\n",
        "\n",
        "This may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)"
      ],
      "metadata": {
        "id": "CsLsZ3H8XukY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_text['lower_text'] = df_text['text'].str.lower()\n",
        "df_text.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:08:16.31814Z",
          "iopub.execute_input": "2025-02-18T10:08:16.318751Z",
          "iopub.status.idle": "2025-02-18T10:08:16.331942Z",
          "shell.execute_reply.started": "2025-02-18T10:08:16.318698Z",
          "shell.execute_reply": "2025-02-18T10:08:16.330358Z"
        },
        "id": "Z0_GqrQhXukY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Removal of HTML Tags¶\n",
        "One another common preprocessing technique that will come handy in multiple places is removal of html tags. This is especially useful, if we scrap the data from different websites. We might end up having html strings as part of our text.\n",
        "\n",
        "First, let us try to remove the HTML tags using regular expressions."
      ],
      "metadata": {
        "id": "R22Dj9unXukZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'',text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:10:39.359217Z",
          "iopub.execute_input": "2025-02-18T10:10:39.359648Z",
          "iopub.status.idle": "2025-02-18T10:10:39.364858Z",
          "shell.execute_reply.started": "2025-02-18T10:10:39.359608Z",
          "shell.execute_reply": "2025-02-18T10:10:39.36332Z"
        },
        "id": "tK0DW2k-XukZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"<div>\n",
        "<h1> H2O</h1>\n",
        "<p> AutoML</p>\n",
        "<a href=\"https://www.h2o.ai/products/h2o-driverless-ai/\"> Driverless AI</a>\n",
        "</div>\"\"\"\n",
        "\n",
        "print(remove_html(text))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:12:09.412046Z",
          "iopub.execute_input": "2025-02-18T10:12:09.412491Z",
          "iopub.status.idle": "2025-02-18T10:12:09.41826Z",
          "shell.execute_reply.started": "2025-02-18T10:12:09.412459Z",
          "shell.execute_reply": "2025-02-18T10:12:09.41696Z"
        },
        "id": "MHTpDopyXuka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Removal of URLs¶\n",
        "Next preprocessing step is to remove any URLs present in the data. For example, if we are doing a twitter analysis, then there is a good chance that the tweet will have some URL in it. Probably we might need to remove them for our further analysis."
      ],
      "metadata": {
        "id": "Nsjo94umXuka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_url(text):\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url_pattern.sub(r'', text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:14:00.150599Z",
          "iopub.execute_input": "2025-02-18T10:14:00.151096Z",
          "iopub.status.idle": "2025-02-18T10:14:00.156623Z",
          "shell.execute_reply.started": "2025-02-18T10:14:00.151056Z",
          "shell.execute_reply": "2025-02-18T10:14:00.155107Z"
        },
        "id": "olC5IaEFXuka"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NLP Future  blog post on https://www.h2o.ai/blog/detecting-sarcasm-is-difficult-but-ai-may-have-an-answer/\"\n",
        "remove_url(text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:15:09.82224Z",
          "iopub.execute_input": "2025-02-18T10:15:09.822682Z",
          "iopub.status.idle": "2025-02-18T10:15:09.830445Z",
          "shell.execute_reply.started": "2025-02-18T10:15:09.822648Z",
          "shell.execute_reply": "2025-02-18T10:15:09.828953Z"
        },
        "id": "usKJ99h8Xukb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Removal of Punctuations¶\n",
        "One another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n",
        "\n",
        "We also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the string.punctuation in python contains the following punctuation symbols\n",
        "\n",
        "!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
        "\n",
        "We can add or remove more punctuations as per our need."
      ],
      "metadata": {
        "id": "KiR9CPAeXukb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_text"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:16:19.552013Z",
          "iopub.execute_input": "2025-02-18T10:16:19.552448Z",
          "iopub.status.idle": "2025-02-18T10:16:19.566829Z",
          "shell.execute_reply.started": "2025-02-18T10:16:19.552416Z",
          "shell.execute_reply": "2025-02-18T10:16:19.565336Z"
        },
        "id": "qWWPdHGQXukb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_text.drop(['lower_text'],axis=1,inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:17:06.743695Z",
          "iopub.execute_input": "2025-02-18T10:17:06.744166Z",
          "iopub.status.idle": "2025-02-18T10:17:06.752187Z",
          "shell.execute_reply.started": "2025-02-18T10:17:06.74413Z",
          "shell.execute_reply": "2025-02-18T10:17:06.750943Z"
        },
        "id": "dkMw6QKeXukc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "PUNCT_TO_REMOVE = string.punctuation\n",
        "def remove_punc(text):\n",
        "    return text.translate(str.maketrans('','',PUNCT_TO_REMOVE))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:20:07.384025Z",
          "iopub.execute_input": "2025-02-18T10:20:07.384448Z",
          "iopub.status.idle": "2025-02-18T10:20:07.389958Z",
          "shell.execute_reply.started": "2025-02-18T10:20:07.384416Z",
          "shell.execute_reply": "2025-02-18T10:20:07.388588Z"
        },
        "id": "mQa9dEj9Xukc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_text['Punc_text'] = df_text['text'].apply(lambda text : remove_punc(text))\n",
        "df_text.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:20:10.18345Z",
          "iopub.execute_input": "2025-02-18T10:20:10.183893Z",
          "iopub.status.idle": "2025-02-18T10:20:10.19813Z",
          "shell.execute_reply.started": "2025-02-18T10:20:10.183858Z",
          "shell.execute_reply": "2025-02-18T10:20:10.196929Z"
        },
        "id": "rxiCo0GeXukc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.Removal of stopwords¶\n",
        "Stopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n",
        "\n",
        "These stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below."
      ],
      "metadata": {
        "id": "6rMy0sDQXukc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "' ,'.join(stopwords.words('english'))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:28:49.280916Z",
          "iopub.execute_input": "2025-02-18T10:28:49.281287Z",
          "iopub.status.idle": "2025-02-18T10:28:49.288708Z",
          "shell.execute_reply.started": "2025-02-18T10:28:49.281258Z",
          "shell.execute_reply": "2025-02-18T10:28:49.287609Z"
        },
        "id": "jjcA1B_1Xukc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = set(stopwords.words('english'))\n",
        "def remove_stopwords(text):\n",
        "\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:28:51.145632Z",
          "iopub.execute_input": "2025-02-18T10:28:51.146092Z",
          "iopub.status.idle": "2025-02-18T10:28:51.153007Z",
          "shell.execute_reply.started": "2025-02-18T10:28:51.146056Z",
          "shell.execute_reply": "2025-02-18T10:28:51.151611Z"
        },
        "id": "oshboUS3Xukc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_text[\"text_stop\"] = df_text[\"Punc_text\"].apply(lambda text: remove_stopwords(text))\n",
        "df_text.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:29:46.565971Z",
          "iopub.execute_input": "2025-02-18T10:29:46.566384Z",
          "iopub.status.idle": "2025-02-18T10:29:46.583517Z",
          "shell.execute_reply.started": "2025-02-18T10:29:46.566351Z",
          "shell.execute_reply": "2025-02-18T10:29:46.582146Z"
        },
        "id": "DeJDINanXukd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Removal of Emojis¶\n",
        "With more and more usage of social media platforms, there is an explosion in the usage of emojis in our day to day life as well. Probably we might need to remove these emojis for some of our textual analysis."
      ],
      "metadata": {
        "id": "AQmKxk_AXukd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:31:57.89802Z",
          "iopub.execute_input": "2025-02-18T10:31:57.898459Z",
          "iopub.status.idle": "2025-02-18T10:31:57.904131Z",
          "shell.execute_reply.started": "2025-02-18T10:31:57.898425Z",
          "shell.execute_reply": "2025-02-18T10:31:57.902746Z"
        },
        "id": "32JTjw80Xukd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "remove_emoji(\"There is 🔥🔥 in the house\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:32:24.666687Z",
          "iopub.execute_input": "2025-02-18T10:32:24.667148Z",
          "iopub.status.idle": "2025-02-18T10:32:24.674371Z",
          "shell.execute_reply.started": "2025-02-18T10:32:24.667112Z",
          "shell.execute_reply": "2025-02-18T10:32:24.673129Z"
        },
        "id": "CYCuFg83Xuke"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.Conversion of Emoji to Words¶\n",
        "Now let us do the same for Emojis as well. Neel Shah has put together a list of emojis with the corresponding words as well as part of his Github repo. We are going to make use of this dictionary to convert the emojis to corresponding words."
      ],
      "metadata": {
        "id": "CcT92TPKXuke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "print(emoji.demojize(\"Python is 🔥😂\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:39:49.347355Z",
          "iopub.execute_input": "2025-02-18T10:39:49.347771Z",
          "iopub.status.idle": "2025-02-18T10:39:49.35384Z",
          "shell.execute_reply.started": "2025-02-18T10:39:49.347738Z",
          "shell.execute_reply": "2025-02-18T10:39:49.352423Z"
        },
        "id": "PBqp4O1fXuke"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.Removal of Frequent words¶\n",
        "In the previos preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us.\n",
        "\n",
        "So this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of.\n"
      ],
      "metadata": {
        "id": "dyHSG_24Xuke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "cnt = Counter()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:34:06.63262Z",
          "iopub.execute_input": "2025-02-18T10:34:06.633258Z",
          "iopub.status.idle": "2025-02-18T10:34:06.638286Z",
          "shell.execute_reply.started": "2025-02-18T10:34:06.633208Z",
          "shell.execute_reply": "2025-02-18T10:34:06.636934Z"
        },
        "id": "GkLRcxpZXuke"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for text in df_text['text_stop'].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "\n",
        "cnt.most_common(20)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:35:28.578597Z",
          "iopub.execute_input": "2025-02-18T10:35:28.579063Z",
          "iopub.status.idle": "2025-02-18T10:35:28.59047Z",
          "shell.execute_reply.started": "2025-02-18T10:35:28.579024Z",
          "shell.execute_reply": "2025-02-18T10:35:28.589216Z"
        },
        "id": "5ZZFLXigXukf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.Stemming¶\n",
        "Stemming is a text preprocessing technique used in Natural Language Processing (NLP) that reduces words to their root form or stem. The goal is to eliminate any suffixes or prefixes to bring related words down to a common base form, which is useful in various NLP tasks like text classification, sentiment analysis, and information retrieval.\n",
        "* running\", \"runner\", \"ran\" all stem to the base form \"run\".\n",
        "* \"better\" and \"good\" might both be reduced to \"good\" (depending on the stemming algorithm).\n",
        "  \n",
        "By reducing words to their root forms, stemming can help models to recognize different forms of a word as the same entity, improving efficiency and effectiveness in downstream tasks."
      ],
      "metadata": {
        "id": "IdKSCrbVXukf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "df_text.drop(['text_stop','Punc_text'],axis=1,inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:44:39.598706Z",
          "iopub.execute_input": "2025-02-18T10:44:39.599176Z",
          "iopub.status.idle": "2025-02-18T10:44:39.60622Z",
          "shell.execute_reply.started": "2025-02-18T10:44:39.599133Z",
          "shell.execute_reply": "2025-02-18T10:44:39.60493Z"
        },
        "id": "-WWD0W0ZXukg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:47:25.088617Z",
          "iopub.execute_input": "2025-02-18T10:47:25.089139Z",
          "iopub.status.idle": "2025-02-18T10:47:25.095319Z",
          "shell.execute_reply.started": "2025-02-18T10:47:25.089098Z",
          "shell.execute_reply": "2025-02-18T10:47:25.09374Z"
        },
        "id": "ciz8TgLiXukg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_text['stemmed_text'] = df_text['text'].apply(lambda text: stem_words(text))\n",
        "df_text.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T10:47:35.83611Z",
          "iopub.execute_input": "2025-02-18T10:47:35.836558Z",
          "iopub.status.idle": "2025-02-18T10:47:35.886254Z",
          "shell.execute_reply.started": "2025-02-18T10:47:35.836524Z",
          "shell.execute_reply": "2025-02-18T10:47:35.885085Z"
        },
        "id": "BX8PzkOaXukg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.Lemmatization\n",
        "Lemmatization is a text preprocessing technique in Natural Language Processing (NLP) that involves converting words into their base or root form, called a lemma. Unlike stemming, which may reduce words to an arbitrary root (sometimes resulting in non-existent words), lemmatization ensures that the base form of the word is a valid word in the language. Lemmatization also takes into account the context and part of speech (POS) of the word to produce the correct base form.\n",
        "\n",
        "* \"running\" becomes \"run\"\n",
        "* \"better\" becomes \"good\"\n",
        "* \"cats\" becomes \"cat"
      ],
      "metadata": {
        "id": "qrezXxRZXukg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize_words(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T11:01:39.865758Z",
          "iopub.execute_input": "2025-02-18T11:01:39.866151Z",
          "iopub.status.idle": "2025-02-18T11:01:40.813361Z",
          "shell.execute_reply.started": "2025-02-18T11:01:39.866119Z",
          "shell.execute_reply": "2025-02-18T11:01:40.812179Z"
        },
        "id": "VhIPS8iLXukg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_text[\"text_lemmatized\"] = df_text[\"text\"].apply(lambda text: lemmatize_words(text))\n",
        "df_text.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T11:01:43.06105Z",
          "iopub.execute_input": "2025-02-18T11:01:43.061501Z",
          "iopub.status.idle": "2025-02-18T11:01:44.212641Z",
          "shell.execute_reply.started": "2025-02-18T11:01:43.061465Z",
          "shell.execute_reply": "2025-02-18T11:01:44.211334Z"
        },
        "id": "nUdH6Eq5Xukq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "nlp(\"running\"),nlp(\"better\"),nlp(\"walking\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T11:03:12.934784Z",
          "iopub.execute_input": "2025-02-18T11:03:12.93527Z",
          "iopub.status.idle": "2025-02-18T11:03:12.96676Z",
          "shell.execute_reply.started": "2025-02-18T11:03:12.935231Z",
          "shell.execute_reply": "2025-02-18T11:03:12.965439Z"
        },
        "id": "OXWgiSPaXukq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.Chat Words Conversion¶\n",
        "This is an important text preprocessing step if we are dealing with chat data. People do use a lot of abbreviated words in chat and so it might be helpful to expand those words for our analysis purposes.\n",
        "\n",
        "Got a good list of chat slang words from this repo. We can use this for our conversion here. We can add more words to this list."
      ],
      "metadata": {
        "id": "v59HVblQXukq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_words = {\n",
        "    'AFAIK': 'As Far As I Know',\n",
        "    'AFK': 'Away From Keyboard',\n",
        "    'ASAP': 'As Soon As Possible',\n",
        "    'ATK': 'At The Keyboard',\n",
        "    'ATM': 'At The Moment',\n",
        "    'A3': 'Anytime, Anywhere, Anyplace',\n",
        "    'BAK': 'Back At Keyboard',\n",
        "    'BBL': 'Be Back Later',\n",
        "    'BBS': 'Be Back Soon',\n",
        "    'BFN': 'Bye For Now',\n",
        "    'B4N': 'Bye For Now',\n",
        "    'BRB': 'Be Right Back',\n",
        "    'BRT': 'Be Right There',\n",
        "    'BTW': 'By The Way',\n",
        "    'B4': 'Before',\n",
        "    'CU': 'See You',\n",
        "    'CUL8R': 'See You Later',\n",
        "    'CYA': 'See You',\n",
        "    'FAQ': 'Frequently Asked Questions',\n",
        "    'FC': 'Fingers Crossed',\n",
        "    'FWIW': 'For What It\\'s Worth',\n",
        "    'FYI': 'For Your Information',\n",
        "    'GAL': 'Get A Life',\n",
        "    'GG': 'Good Game',\n",
        "    'GN': 'Good Night',\n",
        "    'GMTA': 'Great Minds Think Alike',\n",
        "    'GR8': 'Great!',\n",
        "    'G9': 'Genius',\n",
        "    'IC': 'I See',\n",
        "    'ICQ': 'I Seek you (also a chat program)',\n",
        "    'ILU': 'ILU: I Love You',\n",
        "    'IMHO': 'In My Honest/Humble Opinion',\n",
        "    'IMO': 'In My Opinion',\n",
        "    'IOW': 'In Other Words',\n",
        "    'IRL': 'In Real Life',\n",
        "    'KISS': 'Keep It Simple, Stupid',\n",
        "    'LDR': 'Long Distance Relationship',\n",
        "    'LMAO': 'Laugh My A.. Off',\n",
        "    'LOL': 'Laughing Out Loud',\n",
        "    'LTNS': 'Long Time No See',\n",
        "    'L8R': 'Later',\n",
        "    'MTE': 'My Thoughts Exactly',\n",
        "    'M8': 'Mate',\n",
        "    'NRN': 'No Reply Necessary',\n",
        "    'OIC': 'Oh I See',\n",
        "    'PITA': 'Pain In The A..',\n",
        "    'PRT': 'Party',\n",
        "    'PRW': 'Parents Are Watching',\n",
        "    'ROFL': 'Rolling On The Floor Laughing',\n",
        "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
        "    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
        "    'SK8': 'Skate',\n",
        "    'STATS': 'Your sex and age',\n",
        "    'ASL': 'Age, Sex, Location',\n",
        "    'THX': 'Thank You',\n",
        "    'TTFN': 'Ta-Ta For Now!',\n",
        "    'TTYL': 'Talk To You Later',\n",
        "    'U': 'You',\n",
        "    'U2': 'You Too',\n",
        "    'U4E': 'Yours For Ever',\n",
        "    'WB': 'Welcome Back',\n",
        "    'WTF': 'What The F...',\n",
        "    'WTG': 'Way To Go!',\n",
        "    'WUF': 'Where Are You From?',\n",
        "    'W8': 'Wait...',\n",
        "    '7K': 'Sick:-D Laugher'\n",
        "}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T11:23:16.396426Z",
          "iopub.execute_input": "2025-02-18T11:23:16.396885Z",
          "iopub.status.idle": "2025-02-18T11:23:16.404975Z",
          "shell.execute_reply.started": "2025-02-18T11:23:16.396848Z",
          "shell.execute_reply": "2025-02-18T11:23:16.403608Z"
        },
        "id": "N6qrW52xXukq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_conversion(text):\n",
        "    new_text = []\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words:\n",
        "            new_text.append(chat_words[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "\n",
        "    return ' '.join(new_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T11:23:50.142914Z",
          "iopub.execute_input": "2025-02-18T11:23:50.143394Z",
          "iopub.status.idle": "2025-02-18T11:23:50.149524Z",
          "shell.execute_reply.started": "2025-02-18T11:23:50.143353Z",
          "shell.execute_reply": "2025-02-18T11:23:50.14814Z"
        },
        "id": "GUJEmfMkXukr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"AFAIK, I will be AFK for a while\"\n",
        "converted_text = chat_conversion(text)\n",
        "print(converted_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T11:23:51.802248Z",
          "iopub.execute_input": "2025-02-18T11:23:51.802721Z",
          "iopub.status.idle": "2025-02-18T11:23:51.809026Z",
          "shell.execute_reply.started": "2025-02-18T11:23:51.802684Z",
          "shell.execute_reply": "2025-02-18T11:23:51.80746Z"
        },
        "id": "DYtK0tcRXukr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Spelling Correction¶\n",
        "One another important text preprocessing step is spelling correction. Typos are common in text data and we might want to correct those spelling mistakes before we do our analysis."
      ],
      "metadata": {
        "id": "jaJypKUMXukr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "incorrect_text = 'ceertain connditions during swval genrtations are moddified in same mannerr'\n",
        "textblb = TextBlob(incorrect_text)\n",
        "textblb.correct().string"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-18T11:08:19.75384Z",
          "iopub.execute_input": "2025-02-18T11:08:19.754283Z",
          "iopub.status.idle": "2025-02-18T11:08:20.339064Z",
          "shell.execute_reply.started": "2025-02-18T11:08:19.754246Z",
          "shell.execute_reply": "2025-02-18T11:08:20.33765Z"
        },
        "id": "rGZS7V8mXukr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "56l050RcXukr"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}