{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "9!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o44Q3imh6Moj",
        "outputId": "69ec548c-d3b9-4f8a-ddfe-0e0884e50c08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary libraries"
      ],
      "metadata": {
        "id": "E0BKOebrboDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU5WtyU25hbR"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset, load_from_disk\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the device agnostic code\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mi3zKsuk52AK",
        "outputId": "b9c8bb2c-6b38-4744-8681-d689f078579f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Pegasus for Sequence-to-Sequence Tasks\n",
        " **Pegasus** model for sequence-to-sequence tasks such as text summarization. We will load the necessary model and tokenizer, and prepare the model for inference.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gWj-E_a3bdI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model = \"google/pegasus-cnn-dailymail\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "model_pegas = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)"
      ],
      "metadata": {
        "id": "IihdOGKc6Zhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading and tokenizing the dataset"
      ],
      "metadata": {
        "id": "AeTjzECHbw_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds = load_dataset(\"samsum\")"
      ],
      "metadata": {
        "id": "sKAd8qwF6zFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds"
      ],
      "metadata": {
        "id": "yFsJNvD66zB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds['train']['dialog'][1]"
      ],
      "metadata": {
        "id": "jq1YLQaA6y_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds['train']['summary']"
      ],
      "metadata": {
        "id": "WGj-Xy0q7FAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_ds = [len(samsum_ds[split]) for split in samsum_ds]\n",
        "print(f\"Split lengths: {split_ds}\")\n",
        "print(f\"Features: {samsum_ds['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "print(samsum_ds[\"test\"][1][\"dialogue\"])\n",
        "print(\"\\nSummary:\")\n",
        "print(samsum_ds[\"test\"][1][\"summary\"])"
      ],
      "metadata": {
        "id": "lw5RzqRo7Jzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_features(batch):\n",
        "    input_encodings = tokenizer(batch['dialogue'], truncation=True, padding=\"max_length\", max_length=1024)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encoding = tokenizer(batch['summary'], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    batch = {**input_encodings, \"labels\": target_encoding[\"input_ids\"]}\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "zzQ-mBO07JwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds_pt = samsum_ds.map(convert_to_features, batched = True)"
      ],
      "metadata": {
        "id": "qa5aP-1572fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds_pt[\"train\"]"
      ],
      "metadata": {
        "id": "tERtxvNu7JtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds_pt[\"train\"][\"input_ids\"][1]"
      ],
      "metadata": {
        "id": "iPFV6wTi7JrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_ds_pt[\"train\"][\"attention_mask\"][1]"
      ],
      "metadata": {
        "id": "uZaXCj3j7JpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Padding data with datacollatorforseq2seq"
      ],
      "metadata": {
        "id": "dyJLjNSRmte1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegas)"
      ],
      "metadata": {
        "id": "pO7oDUpZ7Jmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the data"
      ],
      "metadata": {
        "id": "O_LWt4Rnm0TV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir = 'pegasus-samsum',\n",
        "    num_train_epochs = 1,\n",
        "    warmup_steps = 500,\n",
        "    per_device_train_batch_size = 1,\n",
        "    per_device_eval_batch_size = 1,\n",
        "    weight_decay = 0.01,\n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'steps',\n",
        "    eval_steps = 500,\n",
        "    save_steps = 1e6,\n",
        "    gradient_accumulation_steps = 16\n",
        ")"
      ],
      "metadata": {
        "id": "bLxr05uP8cH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model = model_pegas,\n",
        "                  args = trainer_args,\n",
        "                  tokenizer = tokenizer,\n",
        "                  data_collator = seq2seq_data_collator,\n",
        "                  train_dataset = samsum_ds_pt[\"test\"],\n",
        "                  eval_dataset = samsum_ds_pt[\"validation\"])"
      ],
      "metadata": {
        "id": "zdCV0fWq8cEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "p7U8qnlw8cCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batch_size_chunks(input_text,batch_size):\n",
        "\n",
        "  for i in range(0, len(input_text), batch_size):\n",
        "    yield input_text[i : i + batch_size]"
      ],
      "metadata": {
        "id": "EXgDQxsA9QL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the data"
      ],
      "metadata": {
        "id": "q3ZonhBnm5Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metric_test(dataset,\n",
        "                          metric,\n",
        "                          model,\n",
        "                          tokenizer,\n",
        "                          device = device,\n",
        "                          batch_size = 16,\n",
        "                          column_text = 'articls',\n",
        "                          column_summary = 'highlights'):\n",
        "\n",
        "  article_batches = list(generate_batch_size_chunks(dataset[column_text],batch_size))\n",
        "  target_batches = list(generate_batch_size_chunks(dataset[column_summary],batch_size))\n",
        "\n",
        "  for article_batches,target_batches in tqdm(\n",
        "      zip(article_batches, target_batches), total = len(article_batches)):\n",
        "\n",
        "      input = tokenizer(article_batch,\n",
        "                        max_length = 1024,\n",
        "                        truncation = True,\n",
        "                        padding = 'max_length',\n",
        "                        return_tensors = 'pt')\n",
        "\n",
        "      summaries = model.generate(input_ids = inputs['input_ids'].to(device),\n",
        "                                 attention_mask = inputs['attention_mask'].to(device),\n",
        "                                 length_penalty = 0.8,\n",
        "                                 num_beams = 8,\n",
        "                                 max_length = 128,\n",
        "                                 length_penalty = 0.8)\n",
        "\n",
        "      decoded_summaries = [tokenizer.decode(s,\n",
        "                                            skip_special_tokens = True,\n",
        "                                            clean_up_tokenization_spaces = True)\n",
        "                            for s in summaries]\n",
        "\n",
        "      decoded_summaries = [d.eplace(\"\",\" \") for d in decoded_summaries]\n",
        "\n",
        "      metric.add_batch(predictions = decoded_summaries,\n",
        "                       references = target_batch)\n",
        "\n",
        "  score = metric.compute()\n",
        "  return score\n"
      ],
      "metadata": {
        "id": "MTNy1zoC9QIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "rouge_metric = load_metric('rouge')"
      ],
      "metadata": {
        "id": "o4Obp6m89QGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",
        ")"
      ],
      "metadata": {
        "id": "aLLiUcCnCYfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model"
      ],
      "metadata": {
        "id": "u_QyAuidm9NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
      ],
      "metadata": {
        "id": "ioaxuuUTCalM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ],
      "metadata": {
        "id": "yeQXSXoICahz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
      ],
      "metadata": {
        "id": "yPR6H56ZCafl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n",
        "gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n",
        "\n",
        "sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n",
        "\n",
        "reference = dataset_samsum[\"test\"][0][\"summary\"]\n",
        "\n",
        "pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n",
        "\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"
      ],
      "metadata": {
        "id": "1iKltTLRCjZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}